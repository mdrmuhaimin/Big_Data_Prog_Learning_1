Under what conditions will the broadcast join be faster than an actual join?
Broadcast create a readonly variable in cache that each executor can access and read from. Usually we broadcast a variable which is
used by multiple executors. So with broadcast join we can load it to cache once and all the executor can access it, without it all the executor needs to have
a copy of it to access it. In our code when we broadcast the average value all the executor get the broadcast value from the cache to calculte
the relative average. Without it when we do the join operations all the executors needs to hold a copy of average value and do the join operations and thus make it slow.
So as longs as we parse the RDD to extract the average and then broadcast it. All the executor will join only the average and thus make the program faster.

When will the broadcast join be slower?
Broadcast join will be slower if not used properly. When we broadcast we need to understand the key portion that each executor needs, and broadcast only that value.
If we broadcase a huge value without parsing and later get the value in executor and parse the data then each executor will work with a bigger size of data and thus make the program slower.
For example we have an array of 100000 element. All we need may be the first 100 element in each executor so in that case we should parse and broadcast first 100 element. So each
executor can work with a smaller dataset.
Another important thing is to broadcast in a stage. For example of amongst this 100 elements only 10 variable needs to be used for the entire execution of code. Other 90 only be used
in some part of the code. The ideal approach would be to broadcast the 90 variable as late as possible and unpersist it as soon as it not needed anymore. However a key
point to remember is not to change the broadcast variable after it is propagated to the nodes.
In our code instead of the average if we broadcast a rdd object without parsing the average from subreddit data it will not be very efficient because in that case each executor
needs to work with this huge data to parse the avg.

What was the problem with the input data wordcount-4? How did you fix the wordcount program so it handled this input and processed it more quickly?
The problem with the input data was it as not evenly sized. So when spark initially read it from the input and created partition, they were handling data with uneven size.
Two files are in our data set are much bigger compare to the other partitions. So two executor was creating a bottleneck because data from the two bigger file was not distributing properly amongst all the
executors. So to fix this problem we need to repartition as soon as we create our RDD. Based on the size of our data set we can do around 120 partition to work with
our data. Our input can be like text = sc.textFile(inputs).repartition(120)

The answers were based on the reading from the following sites
http://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables
https://blog.knoldus.com/2016/04/30/broadcast-variables-in-spark-how-and-when-to-use-them/
https://stackoverflow.com/questions/41580725/what-is-the-disadvantages-of-spark-broadcast-variable
https://stackoverflow.com/questions/37468405/tips-for-properly-using-large-broadcast-variables/37489738#37489738
