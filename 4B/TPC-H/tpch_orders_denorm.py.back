import sys
from pyspark import SparkConf
import pyspark_cassandra
from pyspark_cassandra import Row
from pyspark.sql import SparkSession
from pyspark.sql import SQLContext

input_keyspace = sys.argv[1]
output_keyspace = sys.argv[2]

cluster_seeds = ['127.0.0.1']
# cluster_seeds = ['199.60.17.171', '199.60.17.188']

conf = SparkConf().setAppName('TPCH') \
        .set('spark.cassandra.connection.host', ','.join(cluster_seeds)) \
        .set('spark.dynamicAllocation.maxExecutors', 20)
sc = pyspark_cassandra.CassandraSparkContext(conf=conf)
spark = SparkSession.builder.config(conf=SparkConf()).getOrCreate()

assert sys.version_info >= (3, 5)  # make sure we have Python 3.5+
assert sc.version >= '2.2'  # make sure we have Spark 2.2+

def get_orders(keyspace, split_size=None):
    rdd_order = sc.cassandraTable(keyspace, 'lineitem', split_size=split_size)
    # partition_count = int(rdd_order.count() / 100)  # As we want 100 data per partition
    # rdd_order = rdd_order.repartition(partition_count)
    return rdd_order

def get_orders_parts(rdd_order, keyspace, split_size=None):

    rdd_lineitem_joined_order_partkey = rdd_order.joinWithCassandraTable(keyspace, 'part').\
            on('partkey').\
            select('name').\
            map(lambda row: Row(orderkey=row[0]['orderkey'], name=row[1]['name'].split(' ')))

    return rdd_lineitem_joined_order_partkey


def main():
    rdd_order = get_orders(input_keyspace).cache()
    orders_parts = get_orders_parts(rdd_order, input_keyspace)
    sqlContext = SQLContext(sc)
    df_order = sqlContext.createDataFrame(rdd_order)
    rdd_order.unpersist()
    df_order_part = sqlContext.createDataFrame(orders_parts)
    df_complete = df_order.join(df_order_part, df_order.orderkey == df_order_part.orderkey)
    df_complete.show()

if __name__ == "__main__":
    main()